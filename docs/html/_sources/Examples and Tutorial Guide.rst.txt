Examples and Tutorial Guide
###########################

This section was created with the purpose of providing a comprehensive example that covers all possible analyses with the Phybers package. 
Here, you will find well-commented test codes and data that will allow you to explore many functionalities of the package. 

To make the examples more comprehensive, we have decided to start with the :ref:`testing-data` subsection, 
where we demonstrate how to calculate brain tractography using the DSI Studio software :cite:`DSI-Studio-YehFC-2013`. 
We also provide a tool that allows you to convert tractography from ``TRK`` (used by DSI Studio, TrackVis, among others) or ``TCK`` (used by MRtrix and others) formats to the ``bundles`` format.
Then, in the :ref:`subsec-example` subsection, we provide test data for running each of the showcased examples, divided into :ref:`segmentation-example` and :ref:`clustering-example` 
It's important to note that completing the :ref:`testing-data` subsection is not necessary to try the provided examples. 
You can start directly in the :ref:`data-test-download` subsection to run them. Additionally, for detailed documentation on each Phybers module, we recommend accessing the :ref:`doc-phybers` section.

.. _testing-data:

How to calculate tractography from diffusion weighted images using DSI Studio software
======================================================================================

Currently, various programs enable the calculation of brain tractography. Nevertheless, in this subsection, 
we offer a workflow to obtain brain tractography from diffusion data of a subject in the Human Connectome Project (HCP) database :cite:`Glasser-2013`, utilizing DSI Studio software. 
Subsequently, provide a code to convert the brain tractography from the output format of DSI Studio (``TRK``) to our working format. 
This tool is also employed for converting from the ``TCK`` format to the bundles format, ensuring compatibility with other programs such as MRtrix, among others.

Download and install the DSI Studio software by following the instructions provided on the `DSI Studio website <https://dsi-studio.labsolver.org/download.html>`_ 
To perform tractography, acquire preprocessed diffusion data images from an HCP subject, which you can download from the following website:  `HCP ConnectomeDB <https://db.humanconnectome.org/app/template/Login.vm>`_

Diffusion data pre-processed with the HCP diffusion pipeline, including diffusion weighting (``bvals``) and direction (``bvecs``) in the same file named (``data.nii.gz``), 
a brain mask, a file (``grad_dev.nii.gz``) that can be used to account for gradient nonlinearities during model fitting, and log files of EDDY processing.
Afterward, you have the option to compute brain tractography with DSI. This can be done through the graphical interface or by using command lines. 
If it's for a single subject, I suggest performing it through the graphical user interface.
To adhere to our workflow (see Figure 1), open the graphical user interface and follow the subsequent 3 steps:   

* Step T1: Open Source Images, allows reading diffusion images in ``NIfTI`` or ``DICOM`` format and then creating the ``SRC`` file. 
  In this case, the diffusion images are located in the ``'data.nii.gz'``. 
  Subsequently, skull strip techniques and Eddy Current and motion corrections can be applied. Finally, the ``SRC`` file is created.

* Step T2: Reconstruction, the ``SRC`` file is read. Afterward, the choice of cerebral tractography reconstruction method is available, with GQI being the selected method in this case. 
  Finally, the ``FIB`` file is generated.

* Step T3: Fiber Tracking & Visualization takes the ``FIB`` file and allows you to adjust parameters for fiber tracking. 
  In this case, it performs deterministic tractography with the following fiber tracking parameters: angular threshold = :math:`60^{0}`, step size = 0.5 *mm*, smoothing = 0.5, minimum length = 30 *mm*, 
  maximum length = 300 *mm*, and a tract count of 1.5 million fibers.  To conclude, the tractography can be stored in ``TRK`` format.

.. figure:: _static/pipeline-DSI-Studio.png
   :alt: pipeline-DSI-Studio.png

   Figure 1. Illustrates the workflow used in DSI Studio to generate brain tractography. 
   In Step T1, diffusion images are read from ``NIfTI`` or ``DICOM`` format, motion and distortion corrections are applied, and the ``SRC`` file is then built. 
   Step T2 involves choosing the fiber reconstruction method, leading to the creation of the FIB file. Step T3 enables fiber tracking reconstruction and the saving of tractography in ``TRK`` format.

Conversion of tractography data to working format (TRK o TCK -> bundles)
========================================================================

The following code allows you to convert tractography from the ``TRK`` format (obtained with DSI Studio) to the bundles format. 
Additionally, it facilitates the conversion of tractography from the ``TCK`` format to the bundles format. 
In future updates of Phybers, we will provide support for other formats such as ``VTK``. To test this tool, Downloads the test dataset `link <https://www.dropbox.com/scl/fo/vjq9shi9zk8ka0iy3a8wq/h?rlkey=in68fyr37faa86poq9rt60rur&dl=1>`_ to the ``'Downloads'`` directory on your PC and 
then unzip it with the ``'extract here'`` option. When reviewing the directory, you will find two files: one named ``'tractography_format_test.trk'``, containing 10 thousand brain fibers, 
and the other ``'diffusion_data.nii'``, which is the diffusion image in the subject's acquisition space. 
This image is used to extract the voxel size and the number of slices necessary to apply the affine transformation and convert the tractography from one format to another.
Next, within the ``'Downloads/format_conversion_testing'`` directory, create a text file named ``'format_conversion_tool.py'``, copy the following code, and save it. 

.. code:: python

   # Import for enabling parallel processing:
   from joblib import Parallel, delayed

   # Import of the nibabel library for handling image file:
   import nibabel as nb

   # Import of the NumPy library:
   import numpy as np

   # Import necessary modules from Phybers for writing fiber bundles:
   from phybers.utils import write_bundle

   def apply_affine_bundle_parallel(in_fibers, affine, nthreads):
      """Use parallel processing to apply the affine transformation to each fiber.
      """
      # Use parallel processing to apply the affine transformation to each fiber:
      out_fibers = Parallel(n_jobs=nthreads)(delayed(apply_affine_fiber)(f, affine) for f in in_fibers)

      return out_fibers

   def apply_affine_point(in_point, affine):
      """Apply an affine transformation to a 3D point.
      """
      # Apply the affine transformation to the input point:
      tmp = affine * np.transpose(np.matrix(np.append(in_point, 1)))

      # Extract the transformed point coordinates
      out_point = np.squeeze(np.asarray(tmp))[0:3]

      return out_point

   def get_affine(image_reference):
      """ Get an affine transformation matrix from an image reference.
      """
      # Load the image reference:
      img_reference = nb.load(image_reference)

      # Get the affine transformation matrix from the image reference:
      affine_ = img_reference.affine

      # Ensure all values in the affine matrix are positive:
      affine_ = np.abs(affine_)

      # Apply specific modifications to the diagonal elements and translation vector:
      affine_[0, 0] = -1
      affine_[1, 1] = -1
      affine_[2, 2] = -1
      affine_[0:3, 3] = np.array(img_reference.header.get_zooms()) * np.array(img_reference.header.get_data_shape())

      return affine_

   def apply_affine_fiber(fiber, affine):
      """Apply an affine transformation to a fiber.
      """
      new_fiber = []

      # Apply the affine transformation to each point in the fiber:
      for p in fiber:
         point_transform = apply_affine_point(p, affine)
         new_fiber.append((point_transform))

      return np.asarray(new_fiber, dtype=np.float32)

   def convert_to_bundles(file_in, image_reference, file_out_name):
      """Convert streamlines to bundles and write to file.
      """
      # Get the affine transformation matrix:
      affine = get_affine(image_reference)

      # Load the streamlines from file:
      fibers = nb.streamlines.load(file_in).streamlines

      # Apply the affine transformation to the streamlines in parallel:
      fibers_converted = apply_affine_bundle_parallel(fibers, affine, -1)

      # Write the streamline to bundles format:
      write_bundle(f'{file_out_name}.bundles', fibers_converted)

Now, to convert from TRK to bundles, at the end of the ``'format_conversion_tool.py'`` script, add the following lines:

.. code:: python

  image_reference = 'diffusion_data.nii'
  file_in= 'tractography_format_test.trk'
  file_out_name = 'tractography_format_test_trk'

  convert_to_bundles(file_in, image_reference, file_out_name)

Subsequently, run Python to obtain the converted fibers from the ``TRK`` to bundles format. In the directory ``'Downloads/format_conversion_testing'``, 
you can verify that the fibers were converted if the following two files are created: 
``'tractography_format_test_trk.bundles'`` and ``'tractography_format_test_trk.bundlesdata'``. To convert from ``TCK`` to bundles, follow the same procedure.

.. _subsec-example:

Examples
========

This subsection provides all the necessary test data to run all the functionalities of Phybers. Two main examples, :ref:`segmentation-example` and :ref:`clustering-example` have been created for this purpose. 
The first step would be to obtain the test data from :ref:`data-test-download`, 
and then you can run both examples in any order you prefer, as each one is independent of the other. 
In the Segmentation Example, an illustration is given of how the Utils module (``deform()``, ``sampling()``, 
and ``intersection()``) and the visualization module (``start_fibervis()``) integrate with tractography segmentation using the ``fiberseg()`` algorithm. 
In the Clustering Example, an integrative example is presented that allows combining the Utils module (``postprocessing()``) and 
the visualization (``start_fibervis()``) with the clustering module's algorithms ``hclust()`` and ``ffclust()``. 
Additionally, tools are provided to analyze clustering results, such as obtaining a histogram of clusters with a size greater than 150 and a length between 50 and 60 *mm*.


.. _data-test-download:

Data testing Download
---------------------

To run each of the examples provided below, `download the dataset <https://www.dropbox.com/scl/fo/shibujoaqkov3rda5sse2/h?rlkey=4wto6ycw61ozs60mo5kjpibic&dl=1>`_  to the ``'Downloads'`` directory on your computer, 
and then unzip the data using the ``'extract here'`` option. This will generate the 'testing_phybers' directory, which should contain: 
an ``NIfTI`` image with the nonlinear transformation to deform the tractography to the MNI space (``'nonlinear_transform.nii'``), 
and two sets of brain tractographies with their respective ``'bundles/bundlesdata'`` files. These sets are labeled with the names ``'subject_15e5.bundles/subject_15e5.bundlesdata'`` 
and ``'subject_12e3.bundles/subject_12e3.bundlesdata'``.
Both tractographies correspond to the same test subject; their fibers have a variable number of points and are aligned in the subject's acquisition space. 
The only difference is that the first tractography set has a size of 1.5 millons brain fibers, while the second set has 10 thousand brain fibers. 
To differentiate them, both are named ``'subject'``, followed by the suffix ``'15e5'`` and ``'12e3'``, where ``'e5'`` represents the power of :math:`10^{5}` and ``'e3'`` represents the power of :math:`10^{4}` 
Next, execute the following examples within the ``'testing_phybers'``  directory.

.. _segmentation-example:

Segmentation Example
--------------------

Before applying segmentation, it is crucial that the subject is in the same space as the brain fiber atlas, and that the subject's fibers have 21 points.
To carry out the segmentation example, follow these three steps: 
First, download the test data from the previous subsection  :ref:`data-test-download`.
Second, download the DWM Bundles Atlas (Pamela Guevara 2012) in the subsection :ref:`atlases-download` to the ``'testing_phybers'`` directory and then unzip it with the ``'extract here'`` option. 
You can also use either of the other two SWM atlases provided in the subsection :ref:`atlases-download`.
Third, create a script file with the name ``'testing_fiberseg.py'``, copy the following command lines, save, and execute in a Python terminal within the ``'testing_phybers'`` directory.

.. code:: python

   # Import deform  sub-module. Transforms a tractography file to another space using a non-linear deformation image file:
   from phybers.utils import deform

   # Import sampling  sub-module. Performs a sampling of the fibers points:
   from phybers.utils import sampling

   # Import fiberseg sub-module. White matter fiber bundle segmentation algorithm based on a multi-subject atlas:
   from phybers.segment import fiberseg

   #Execute the deform function:
   deform ( deform_file = 'nonlinear_transform.nii', file_in = 'subject_15e5.bundles', file_out = 'subject_15e5_MNI.bundles')

   # Execute the sampling function:
   sampling ( file_in = 'subject_15e5_MNI.bundles', file_out = 'subject_15e5_MNI_21pts.bundles', npoints = 21 )

   # Path to the tractography dataset file to segment:
   file_in = 'subject_15e5_MNI_21pts.bundles'

   # Subject name used to label the results; in this case, the suffix '15e5' is used:
   subj_name = '15e5'

   # Directory to the bundles atlas:
   atlas_dir = 'DWM_atlas_2012/bundles'

   # Path to the atlas information file:
   atlas_info = 'DWM_atlas_2012/atlas_info.txt'

   # Directory to save all result segmentation, in this case 'seg_result':
   dir_out = 'seg_result'

   # Execute the fiberseg function:
   fiberseg(file_in, subj_name, atlas_dir, atlas_info, dir_out)

Visualization of segmented fascicles
************************************

The first step to verify that the segmentation was performed correctly would be to open the ``'seg_result'`` directory and check that the following 3 directories were created:

* ``'seg_result/final_bundles'``: It contains separately all the segmented fascicles in ``'bundles/bundlesdata'`` files, sampled at 21 points and in the space of the atlas (MNI). 
  For example, for the inferior longitudinal fascicle (IL), it should have the following label as the name ``'15e5_to_IL_LEFT_MNI.bundles/15e5_to_IL_LEFT_MNI.bundlesdata'``. 
  This name encodes that we are dealing with the segmentation result for the subject ``'15e5'``, specifically for the inferior longitudinal fascicle (IL) in the left hemisphere (LEFT), 
  and it is located in the MNI space. This idea holds for the rest of the fascicles.

* ``'seg_result/bundles_id'``: It is a text file containing, for each segmented bundle, the indices of the fibers in the subject's tractography dataset file. 
  The name of each file follows the same structure as explained earlier. 
  These files are used to obtain the segmented fascicles in the subject's acquisition space and with all points of brain fibers.

* ``'seg_result/centroids'``: A directory containing the centroid of each segmented fascicle, saved in a single file in bundles format, 
  named ``'centroids.bundles/centroids.bundlesdata'``, sampled at 21 points and in the space of the atlas (MNI).


The second step to check the segmentation results would be to use our visualization software to observe the segmented fascicles. 
To do this, execute the following commands in the ``'Downloads/testing_phybers'`` directory:

.. code:: python

   # Import fibervis:
   from phybers.fibervis import start_fibervis

   # Execute the graphical user interface for fibervis:
   start_fibervis()

Subsequently, navigate to the directory that stores all segmented bundles, in this case, ``'seg_result/final_bundles'``. 
Select the 3 segments of the arcuate fasciculus (AR) for the left hemisphere: ``15e5_to_AR_ANT_LEFT_MNI.bundles``, ``15e5_to_AR_ANT_RIGHT_MNI.bundles``, and ``15e5_to_AR_LEFT_MNI.bundles``. 
This will allow you to obtain an image similar to the one shown in Figure 2.

.. figure:: _static/ARF-seg.png
   :alt: ARF-seg.png

   Figure 2. Bundle segmentation results for a subject using the DWM Bundles Atlas :cite:`PGuevara-2012` 
   Three segments of the arcuate fasciculus in the left hemisphere: direct segment (purple), anterior segment (yellow), and posterior segment (green).

Retrieve the fascicles with all points in the subject's acquisition space
*************************************************************************

In the ``'seg_result/final_bundles'`` directory, all detected fascicles in the test subject are stored. 
These fascicles have the same number of fiber points, specifically, 21 points, and are located in the atlas space. 
In case you need to obtain the fascicles in the same acquisition space as the subject and with the same number of points as the original,
execute the following commands on the ``'testing_phybers'`` directory.

.. code:: python

   # Import Python libraries for additional processing steps:
   import os
   import numpy as np

   # Import necessary Phybers modules for fiber reading and writing, respectively:
   from phybers.utils import read_bundle, write_bundle

   # Reads the fibers with all points and in the subject's acquisition space:
   bundles_allpoint = read_bundle('subject_15e5.bundles')

   # Convert the bundle list into a NumPy array
   bundles_allpoint_to_array = np.asarray(bundles_allpoint, dtype=object)

   # Create a directory to store the final bundles with all points
   directory = os.path.join ('seg_result', 'final_bundles_allpoints')
   if not os.path.exists(directory):
      os.mkdir(directory)

   # Iterate through files in the 'bundles_id/bundles_id' directory
   for file_id in os.listdir(os.path.join('seg_result', 'bundles_id')):
      indices=[]

      # Read indices from the file
      with open(os.path.join('seg_result', 'bundles_id',file_id), 'r') as f:
         for line in f:
               indices.append(int(float(line.strip())))

      # Write the fibers with all points to a new bundle file
      write_bundle(os.path.join('seg_result','final_bundles_allpoints',file_id[:-4]+'.bundles'),bundles_allpoint_to_array[indices])

Calculating intersection between fascicles
******************************************

The ``phybers.utils`` module includes the ``'intersection'`` tool, designed to calculate the percentage of overlap between two given fascicles. 
This tool proves valuable for evaluating the impact of linear registration versus nonlinear registration on segmented data, as outlined in :cite:t:`Claudio-Roman-2017`. 
Additionally, it can be employed to identify similarities between the same fascicle across two different subjects. 
In this instance, the proposal is to compute the intersection for the fascicle inferior longitudinal fascicle for the left hemisphere, 
acquired after segmentation in the previous stage. Thus, ``'15e5_to_IL_LEFT_MNI.bundles'`` will be passed as an argument for both file1_in and file2_in. 
This allows us to anticipate a 100% intersection result.

To execute an example of ``phybers.utils.intersection()`` on the recently obtained segmentation results, open a Python terminal in the ``'testing_phybers'`` directory and run the following commands:

.. code:: python

   # Import postprocessing:
   from phybers.utils import intersection

   # Import Python library
   import os

   # Execute the intersection function:
   result_intersection = intersection ( file1_in = os.path.join('seg_result', 'final_bundles', '15e5_to_IL_LEFT_MNI.bundles'),
                                    file2_in = os.path.join('seg_result', 'final_bundles', '15e5_to_IL_LEFT_MNI.bundles'),
                                    dir_out = os.path.join('seg_result', 'distance_matrix'), distance_thr = 10.0 )

   # Display the results of intersection
   print(' Intersection fibers1 with fibers2 ', result_intersection [0] )
   print(' Intersection fibers2 with fibers1 ', result_intersection [1] )

.. _clustering-example:

Clustering Example
------------------

The clustering module includes two clustering algorithms. 
First, an example of how to apply ``'phybers.clustering.hclust()'`` will be presented, followed by an example of ``'phybers.clustering.ffclust()'``. 
Before carrying out any of the provided examples, download the test data following the instructions given in section :ref:`data-test-download`.


HClust
******

Navigate to the ``'Downloads/testing_phybers'`` directory, open a Python terminal, and execute the commands provided below.

.. code:: python

   # Import hclust sub-module. Average-link hierarchical agglomerative clustering algorithm:
   from phybers.clustering import hclust

   # Path to the tractography dataset file to hclust:
   file_in = 'subject_12e3.bundles'

   # Directory to save all result clustering:
   dir_out = 'hclust_result'

   # Maximum distance threshold in mm: 
   fiber_thr = 70

   # Adaptive partition threshold in mm:
   partition_thr = 70

   # Variance squared and provides a similarity scale in mm:
   variance = 3600

   # Execute the hclust function:
   hclust ( file_in, dir_out, fiber_thr, partition_thr, variance )

Visualization of the clusters obtained with HClust
""""""""""""""""""""""""""""""""""""""""""""""""""
If hclust executed successfully in ``'Downloads/testing_phybers/hclust_result'``, you should be able to verify the existence of the following entries:

* ``'final_bundles'``: Directory that stores all the fiber clusters found in different ``'bundles/bundlesdata'`` files. 
  The file names are labeled with integer numbers ranging from zero to the total number of fiber clusters found.

* ``'centroids'``: Directory that contains the centroid for each created cluster in same 'bundles/bundlesdata' files. 
  The firt fiber of bundle centroid is corresponding with centroid calculated for cluster one and so on.

* ``'bundles_id'``:Text file that stores the fiber index input for each of the detected clusters. The fiber indexes are extracted from the tractography input. 
  The first line corresponds to cluster zero and so on.

* ``'outputs'``: Temporal directory with intermediate results.

Additionally, you can use the visualization module to observe and interact with the obtained results. 
First, run the fibervis graphical user interface, and then open the files located in the ``'Downloads/testing_phybers/hclust_result/final_bundles'`` directory.
To execute fibervis, open a Python terminal in the ``'Downloads/testing_phybers'`` directory and run the following commands:

.. code:: python

   # Import fibervis:
   from phybers.fibervis import start_fibervis

   # Execute the graphical user interface for fibervis:
   start_fibervis()


.. figure:: _static/figure-hclust.png
   :alt: figure-hclust.png

   Figure 3. Results of applying FFClust to the whole-brain tractography dataset with 1.5 million streamlines. 
   The detected clusters were filtered using the PostProcessing submodule of the Utils module; 
   the filtering criterion shows clusters with a size greater than 150 and a length between 50 mm and 60 mm.

FFClust
*******

To execute ffclust, find the directory on your computer ``'Downloads/testing_phybers'``. Afterward, open a Python terminal and run the following commands:

.. code:: python

   # Import the ffclust sub-module. Intra-subject clustering algorithm: 
   from phybers.clustering import ffclust

   # Import the ffclust sub-module. Intra-subject clustering algorithm:
   from phybers.utils import sampling

   # Execute the sampling function:
   sampling ( file_in = 'subject_15e5.bundles', file_out = 'subject_15e5_21pts.bundles', npoints = 21 )

   # Path to the tractography dataset file with 21 points:
   file_in = 'subject_15e5_21pts.bundles'

   # Directory to save all segmentation results: 
   dir_out = 'ffclust_result'

   # Indices of the points to be used in point K-means:
   points = [0, 3, 10, 17, 20]

   # Number of clusters to be computed for each point using K-Means:
   ks = [200, 300, 300, 300, 200]

   # Maximum distance threshold for the cluster reassignment in mm:
   assign_thr = 6

   # Maximum distance threshold for cluster merging in mm:
   join_thr = 6

   # Execute the ffclust function:
   ffclust (file_in, dir_out, points, ks, assign_thr, join_thr)


FFClust results directory
"""""""""""""""""""""""""

Once ffclust has been successfully executed in ``'Downloads/testing_phybers/ffclust_result'``, you should be able to confirm the presence of the following entries that have the same structure as HClust:

* ``'final_bundles'``: Directory that stores all the fiber clusters found in different ``'bundles/bundlesdata'`` files. 
  The file names are labeled with integer numbers ranging from zero to the total number of fiber clusters found.

* ``'centroids'``: Directory that contains the centroid for each created cluster in same 'bundles/bundlesdata' files. 
  The firt fiber of bundle centroid is corresponding with centroid calculated for cluster one and so on.

* ``'bundles_id'``:Text file that stores the fiber index input for each of the detected clusters. The fiber indexes are extracted from the tractography input. 
  The first line corresponds to cluster zero and so on.

* ``'outputs'``: Temporal directory with intermediate results.

Moreover, you have the choice to use the visualization module to examine and interactively work with the results obtained. 
Start by initiating the fibervis graphical user interface, and then access the files located in the ``'Downloads/testing_phybers/ffclust_result/finalbundles'`` directory. 
To execute fibervis, open a Python terminal in the ``'Downloads/testing_phybers'`` directory and run the commands provided below:



Filtering the clusters with a size greater than 150 and a length in the range from 50 to 60 mm
""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""

Useful tool for exploring and evaluating segmentation and clustering results is ``postprocessing()`` of utils module. 
This tool extracts information from clusters, such as size, average length, and intra-cluster distance. 
It can then perform various types of filtering, write data, and build a histogram with data distribution. 
Following is usage of postprocessing submodule on result obtained after applying ``ffclust()``. 
First, open Python terminal in ``'Downloads/testing_phybers/ffclust_result'`` directory, and then execute following command lines. 
It's important to note that each code segment presented should be executed sequentially, as from now on, each code depends on the previous one. 
Alternatively, you can copy everything into a Python script and run it all at once.

.. code:: python

   # Import postprocessing:
   from phybers.utils import postprocessing

   # Import necessary Phybers modules for fiber reading and writing, respectively:
   from phybers.utils import read_bundle, write_bundle

   # Import Python libraries for additional processing steps:
   import numpy as np
   import pandas as pd
   import os

   # Set the input directory
   dir_in = 'ffclust_result'

   # Execute postprocessing:
   df_metrics = postprocessing (dir_in)

The function ``df_metrics = postprocessing(dir_in)`` returns a ``Pandas DataFrame`` object that contains the following list of keys: 
``'size'`` (number of fibers in the bundle), ``'len'`` (centroid length per bundle), and ``'intra_min'`` (minimum intra-bundle Euclidean distance). 
The cluster identifier (hablar)

This allows for multiple filters to be applied to the cluster results by manipulating such a well-known data structure as Pandas. 
The calculated DataFrame is automatically stored in the directory ``'ffclust_result/outputs/mensures/mensures.xlsx'``.

The following code aims to filter the ``DataFrame`` stored in df_metrics and then write the detected clusters with the same number of points as the original brain tractography 
(which contains a variable number of fibers). 
As filtering criteria, it is defined to select clusters with a size greater than or equal to 150 and a length greater than or equal to 40 *mm* but less than or equal to 60 *mm*. 
To do this, keep the same directory and execute the following commands:

.. code:: python

   # Filtering the clusters with a size greater than 150 and a length in the range from 50 to 60 mm:
   filter_df_metrics = df_metrics[(df_metrics['size'] >= 150) & (df_metrics['len'] >= 40) & (df_metrics['len'] <= 60)]

   # List with the indices of filtered fibers:
   list_id_fibers_filter = list(filter_df_metrics.index)

   # Reads the fibers with all points and in the subject's acquisition space:
   bundles = read_bundle('subject_15e5.bundles')

   # Convert bundles to a NumPy array:
   bundles_to_array = np.asarray(bundles, dtype=object)

   # Define the output directory for filtered bundles:
   directory = os.path.join('ffclust_result', 'filter_150S_40-60L')

   # Create the directory if it doesn't exist:
   if not os.path.exists(directory):
      os.mkdir(directory)

   # Read cluster indices from a file and extract indices
   dir_id_clusters = os.path.join('ffclust_result', 'bundles_id.txt')

   # Initialize a counter to store the cluster numbers:
   cont=0

   # Initialize a dictionary to store the numbers and indices of the clusters:
   dicc = {}

   # Read cluster indices from the file and store them in a dictionary:
   with open(dir_id_clusters, 'r') as f:

      for i in f.readlines():

         # Initialize an empty list to store the indices of the clusters:
         indices_clusters = []
         dicc [cont] = list(map(int, i.split('-')[1].split(' ')[:-1]))
         cont+=1

   # Write filtered bundles to separate files based on their indices:        
   for i in list_id_fibers_filter:   
      write_bundle(os.path.join(directory,str(i)+'.bundles'),bundles_to_array[dicc[i]])


Visualization of the filtered clusters
""""""""""""""""""""""""""""""""""""""

The filtered clusters are stored in the ``'Downloads/testing_phybers/ffclust_result/filter_150S_40-60L'`` directory.
To visualize them, run the graphical user interface of fibervis using the following command lines and then drag all the bundle files. 
This will allow you to obtain an image similar to the one shown in Figure 3.

.. code:: python

   # Import fibervis
   from phybers.fibervis import start_fibervis

   # Execute the graphical user interface for fibervis:
   start_fibervis()


.. figure:: _static/filter_ffclust_150S_50-60L.png
   :alt: filter_ffclust_150s_50-60l.png

   Figure 4. Results of applying FFClust to the whole-brain tractography dataset with 1.5 million streamlines. 
   The detected clusters were filtered using the PostProcessing submodule of the Utils module; 
   the filtering criterion shows clusters with a size greater than 150 and a length between 50 mm and 60 mm.

Histogram of the filtered clusters
""""""""""""""""""""""""""""""""""
Below is an example that allows you to calculate a histogram to observe the filtered clusters (Figure 4 shows the expected result):

.. code:: python

   # Import Seaborn and Matplotlib for creating the histogram:
   import seaborn as sns
   import matplotlib.pyplot as plt

   # Set the bin width for the histogram:
   bin_width = 100

   # Find the maximum and minimum values of the 'size' column in the DataFrame:
   max_lens = filter_df_metrics['size'].max()
   min_lens = filter_df_metrics['size'].min()

   # Create bins with the specified bin width, covering the range from the minimum to maximum 'size' values:
   bins = range(min_lens, max_lens + bin_width, bin_width)

   # Create a histogram plot using Seaborn, specifying the data, the 'size' column, bins, and visual settings:
   sns.histplot(data=filter_df_metrics, x='size', bins=bins, kde=False, edgecolor='black')

   # Set the title and labels for the plot:
   plt.title('Clusters with a size greater than 150 and a length ranging from 50 to 60 *mm*')
   plt.xlabel('Sizes')
   plt.ylabel('Frequency')

   # Adjust x-axis tick labels with rotation, alignment, and font size:
   plt.xticks(bins, rotation=45, ha='right', fontsize=8)

   # Add grid lines on the y-axis with a dashed linestyle and reduced alpha (transparency):
   plt.grid(axis='y', linestyle='--', alpha=0.7)

   # Display the histogram plot:
   plt.show()


.. figure:: _static/histplot_sizes.png
   :alt: filter_ffclust_150s_50-60l.png
   
   Figure 5. Histogram constructed to display the sizes of those obtained with FFClust that have a size greater than 150 and a length between 50 mm and 60 mm.

